---
title: "Untitled"
author: "Anne Valder"
date: "2/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# libraries

```{r, include=FALSE}
library(readr)
library(readxl)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tseries)
library(forecast)
library(vars)
library(GGally)
```


```{r}
rm(list=ls())
getwd()
setwd("/Users/annevalder/Desktop/UNI /WU WIEN/SoSe_21/MA/R-CODE")
```
# Load all data in (GDP, CCI, Goolge), extension unemployment (statAT), inflation(statAT), uncertainty index (paper Brown et al)

#GDP
```{r}
t_0 <- as.Date("2006-01-01") #Setting start date for the series
t_1 <- as.Date("2021-07-01") #Setting end date for the series
t_2 <- as.Date("2021-12-31")
Date_q <- seq.Date(t_0,t_1,by="quarter")
Date_m <- seq.Date(t_0,t_2,by="month")

bigT <- length(Date_q)
M <- 6
Data   <- matrix(NA,bigT,M)
colnames(Data)  <- c("gdp", "cci", "google","gdp_ts", "cci_ts", "google_ts") 
Data <- as.data.frame(Data)
rownames(Data)  <- as.Date(Date_q)

```


```{r}
# abgerufen final am 11.02.2022
#Source: https://data.oecd.org/gdp/quarterly-gdp.htm#indicator-chart
gdp <- read.csv("https://raw.githubusercontent.com/anneval/MA_data/main/RData/GDP_oecd.csv")[9:71,6:7]

colnames(gdp)[colnames(gdp) %in% c("TIME", "Value")] <- c("QDate", "value")
gdp <- cbind(gdp,Date_q)
#ts:
gdp_ts <- ts(gdp$value, start=c(2006,1), end = c(2021,3), frequency = 4)

Data[(Date_q%in%gdp$Date_q),1] <- gdp$value
Data$gdp_ts <- ts(Data$gdp, start=c(2006,1), end = c(2021,3), frequency = 4)
```
#CCI
```{r, include=FALSE}
# abgerufen final am 11.02.2022
#Source: https://ec.europa.eu/info/business-economy-euro/indicators-statistics/economic-databases/business-and-consumer-surveys/download-business-and-consumer-survey-data/time-series_en#consumers

cci <- read.csv2("https://raw.githubusercontent.com/anneval/MA_data/main/RData/CCI.csv")[253:444,c(1,297)]

colnames(cci)[colnames(cci) %in% c("X", "CONS.AT.TOT.COF.BS.M")] <- c("time", "value")
cci <- cbind(cci,Date_m)
cci <- arrange(cci, Date_m)
cci$Date_q <- as.yearqtr(cci$Date_m) 
cci$Date_q <- as.Date(cci$Date_q)

#cci$Date_q  <- as.character(cci$Date_m)
#cci$Date_q[CCI_mntly$Date_q == "2021 Q4"] <- "2021 Q3"

cci_qtrly <- cci %>% group_by(Date_q) %>%
  summarise_all(mean)

cci_qtrly_63 <- cci_qtrly[1:63,]
Data[(Date_q%in%cci_qtrly_63$Date_q),2] <- (cci_qtrly_63$value)
Data$cci_ts <- ts(Data$cci, start=c(2006,1), end = c(2021,3), frequency = 4)

```

# Google
```{r}
# abgerufen: daily update
Google_AT_daily <- read.csv("https://raw.githubusercontent.com/anneval/MA_data/main/raw/at/trendecon_sa.csv")

# transform to quarterly and then do ts:
Google_AT_daily$time <- as.Date(Google_AT_daily$time)
Google_AT_daily <- arrange(Google_AT_daily, time)
Google_AT_daily$Date_q <- as.yearqtr(Google_AT_daily$time)
Google_AT_daily$Date_q <- as.Date(Google_AT_daily$Date_q)
#Google_AT_daily$QDate  <- as.character(Google_AT_daily$QDate)
#Google_AT_daily$QDate[Google_AT_daily$QDate == "2022 Q1"] <- "2021 Q3"  
#Google_AT_daily$QDate[Google_AT_daily$QDate == "2021 Q4"] <- "2021 Q3"  
Google_AT_qtrly <- Google_AT_daily %>% group_by(Date_q) %>%
  summarise_all(mean)

#rownames(Google_AT_qtrly)  <- as.Date(Date_q)
Google_AT_qtrly_63 <- Google_AT_qtrly[1:63,]
Data[(Date_q%in%Google_AT_qtrly_63$Date_q),3] <- (Google_AT_qtrly_63$value)
Data$google_ts <- ts(Data$google, start=c(2006,1), end = c(2021,3), frequency = 4)
```

```{r}
cci_test <- as.data.frame(cci_qtrly[,c(1,3)])
```

#ADF tests: 
```{r}
adf.test(Data$gdp) # stationary
adf.test(Data$cci) # stationary
adf.test(Data$google) # not stat # test agao  after remove inlation 
# not stationary since I included inflation... 

adf.test(diff(Data$gdp))
adf.test(diff(Data$cci))
adf.test(diff(Data$google))
```
kss tests and phillip peron tests:
```{r}
kpss.test(Data$gdp) # stationary
kpss.test(Data$cci) # stationary
kpss.test(Data$google) # stationary

pp.test(Data$gdp) # stationary
pp.test(Data$cci) # not stationary
pp.test(Data$google) # stationary
```
# ACF and PACF plots: Reasoning stationarity
```{r}
par(mfrow = c(2,2))
acf(Data$gdp)
acf(Data$cci)
acf(Data$google)

par(mfrow = c(2,2))
pacf(Data$gdp)
pacf(Data$cci)
pacf(Data$google)
```
# Correlations
```{r}
ggpairs(Data[,1:3])
```

#### Models - simple first later expanding? (and CV, robustness)

```{r}
Data_train <- Data[1:44,] # 70%
Data_test <- Data[45:63,] #20%


Data_train <- Data[1:12,] 
Data_test <- Data[13:63,]


Data_train <- Data[1:40,] 
Data_test <- Data[41:63,]
```

# Models Heik:

AR(1)
Model 1: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \epsilon_t  \qquad  t =1,\dotsc,T$

```{r}
mod1 <- lm(gdp ~ lag(gdp), data = Data_train)
pred_mod1 <- predict(mod1, Data_test)
pred_mod1
summary(mod1)
accuracy(mod1)
#forecast_mod1 <- predict(mod1, n.ahead = 3)
#plot(forecast_mod1)
```
Model 2:  $GDP_t = \beta_0 +\beta_1 CCI_{t} +\epsilon_t \qquad  t =1,\dotsc,T$
```{r}
mod2 <- lm(gdp ~ cci, data = Data_train)
pred_mod2 <- predict(mod2, Data_test)
#pred_test <- predict(mod2, cci_test)
#mod2$coefficients

summary(mod2)
accuracy(mod2)
```
Model 3:  $GDP_t = \beta_0 +\beta_1 GT_{t,i} +\epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$

```{r}
mod3<- lm(gdp ~ google, data = Data_train)
pred_mod3 <- predict(mod3, Data_test)
summary(mod3)
accuracy(mod3)
```
Model 4: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 CCI_{t} + \epsilon_t \qquad  t =1,\dotsc,T$

```{r}
mod4<- lm(gdp ~ lag(gdp) + cci, data = Data_train)
pred_mod4 <- predict(mod4, Data_test)
summary(mod4)
accuracy(mod4)
```
Model 5: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 GT_{t,i} + \epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$

```{r}
mod5<- lm(gdp ~ lag(gdp) + google, data = Data_train)
pred_mod5 <- predict(mod5, Data_test)
pred_mod5
summary(mod5)
accuracy(mod5)
```

Model 6: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 CCI_{t} + \beta_3 GT_{t,i} + \epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$
```{r}
mod6<- lm(gdp ~ lag(gdp) + cci + google, data = Data_train)
pred_mod5 <- predict(mod6, Data_test)
summary(mod6)
accuracy(mod6)
```

#VAR model mit Varselect all 3 variables
```{r}
Yselect <- VARselect(Data_train[,1:3])      #lag 10         # computes information criteria
  lag <- Yselect$selection[1]  # AiC
  
VAR_est <- VAR(y = Data_train[,1:3], p = lag)
#summary(VAR_est$varresult$gdp)$adj.r.squared
#summary(VAR_est$varresult$cci)$adj.r.squared
#summary(VAR_est$varresult$google)$adj.r.squared
forecasts <- predict(VAR_est,Data_test[,1:3])
forecasts
plot(forecasts)

```
# Residual analysis
```{r}
sum2 <- data.frame(
  VAR_est$varresult$gdp$residuals,
  VAR_est$varresult$cci$residuals,
  VAR_est$varresult$google$residuals)

adf.test( VAR_est$varresult$gdp$residuals) # stationary
adf.test(VAR_est$varresult$cci$residuals) #stationary
adf.test(VAR_est$varresult$google$residuals)#stationary
```

# Normality test
```{r}
Normtest <- normality.test(VAR_est) # H0: normality
print(Normtest)
plot(Normtest)

```
# serial correlation check
```{r}
ser.test <- serial.test(VAR_est) # H0: no serial correlation
ser.test
```

#VAR model mit Varselect gdp and cci 
```{r}
Yselect_2 <- VARselect(Data_train[,1:2])               # computes information criteria
  lag <- Yselect_2$selection[1]  
  
VAR_est_2 <- VAR(y = Data_train[,1:2], p = lag)
#summary(VAR_est$varresult$gdp)$adj.r.squared
#summary(VAR_est$varresult$cci)$adj.r.squared
#summary(VAR_est$varresult$google)$adj.r.squared
forecasts_2 <- predict(VAR_est_2,Data_test[,1:2])

plot(forecasts_2)

```

#VAR model mit Varselect gdp and google 
```{r}
Yselect_3 <- VARselect(Data_train[,c(1,3)])               # computes information criteria
  lag <- Yselect_3$selection[1]  
  
VAR_est_3 <- VAR(y = Data_train[,c(1,3)], p = lag)
#summary(VAR_est$varresult$gdp)$adj.r.squared
#summary(VAR_est$varresult$cci)$adj.r.squared
#summary(VAR_est$varresult$google)$adj.r.squared
forecasts_3 <- predict(VAR_est_3,Data_test[,c(1,3)])
#accuracy(forecasts)

plot(forecasts_3)

```

# AR & VAR model Heik again mit selction criteria (AIC, autoarima, varselect)
```{r}
# AR(1) model 
arima <- auto.arima(Data_train[,"gdp"], trace = TRUE,  ic = c("aic"))
#pred <- predict(fit,Data_test) ?? 

pred <- predict(arima)

plot(forecast(arima,h=3)) # forecast 1 step ahead!!! 
accuracy(arima)
```
# do grid and then different arima models and compare by AIC
(0,0,0) loop - later if time... 
```{r}
# works only problem is how do i safe each ar model to each row in list ar.. [[]] maybe one more loop und length(..)

grid_test <- expand.grid(p = seq(0, 4, by= 1),
                            q= seq(0, 4, by= 1),
                                   d = seq(0, 4, by= 1))

ar_model <- function(data){
  ar <- list()
  for (p in 1:2) for (d in 1:2) for (q in 1:2) {
  ar[] <- arima(data[,1],order = c(p,d, q),include.mean = TRUE)
  #print(c(p,d,q))
}
  return(ar)
}
test <- ar_model(data = Data_train)

```
#Extensions
```{r}
# maybe policyy uncertainty index
# unemployment and inflation VAR 
```


# Expanding window estimation instead of fixed test and train data set 
# average over all 

```{r}
expanding_window_OLS <- function(data, dep_var, start = 13){ # change to 12 
  expanding_OLS <- list() # empty vector
  predicted_OLS <- list()
  resid_OLS <- list()
  error_OLS <- c() 
  i <- 0
  for(t in start:nrow(data)){#t in 13:51
    i <- i+1
    expanding_OLS[[i]] <- lm(formula = model, data = data[1:(t-1),]) # start at 13-1 so 12! 
    resid_OLS[[i]] <- resid(expanding_OLS[[i]])
    predicted_OLS[[i]] <- predict(expanding_OLS[[i]],newdata = data[t,])#[t,])
    error_OLS[i]  <- as.numeric(predicted_OLS[[i]] - data[t,]) # anpassen je nach dem welchs
    Summary_OLS <- list(expanding_OLS,predicted_OLS,resid_OLS,error_OLS)
  }
  return(Summary_OLS)
}
```

#model1 expanding estimation
```{r}
dep_var <- "gdp"
model = gdp ~ lag(gdp)

gdp_lag <- lag(Data$gdp)
gdp_lag <- as.data.frame(gdp_lag)

expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13)
pred <- do.call(rbind.data.frame, expanding_OLS[[2]]) # NAS wegen lag! 

### error in and OOS 
error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS  <- sqrt(mean(error_OLS^2)) 
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) 
MSE_OLS
```

#model2 again
```{r}
model = gdp ~ cci
expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13)

pred <- do.call(rbind.data.frame, expanding_OLS[[2]]) 
colnames(pred) <- c("value")
rownames(pred) <- Date_q_pred
Date_q_pred <- Date_q[13:63]

pred <- as.ts(pred, start=c(2009,1), end = c(2021,3), frequency = 4)
ts.plot(pred)

### error in and OOS 
error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2)) 
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) 
MSE_OLS
```

#model3 again
```{r}
model = gdp ~ google
expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13)

pred <- do.call(rbind.data.frame, expanding_OLS[[2]]) 
colnames(pred) <- c("value")
rownames(pred) <- Date_q_pred
Date_q_pred <- Date_q[13:63]

pred <- as.ts(pred, start=c(2009,1), end = c(2021,3), frequency = 4)
ts.plot(pred)

### error in and OOS 
error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2)) 
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) 
MSE_OLS
```

#model4 again
```{r}
model = gdp ~ lag(gdp) + cci
expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13)

```

#model5 again
```{r}
model = gdp ~ lag(gdp) + google
expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13)

```

#model6 again
```{r}
model = gdp ~ lag(gdp) + cci + google
expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13)


```


# write down all models  I want todo (Heik models, 4 lags quarterly data, and tests for lag length with arima and VAR models)

# do simple split OOS forecast and expanding OOS forecast
# adjust expanding window functions (for Var, arima)
# Forecasting with rest of the data... 
# Robustness different aggregation,  LLOCV
