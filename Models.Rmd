---
title: "Untitled"
author: "Anne Valder"
date: "2/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# libraries

```{r}
library(readr)
library(readxl)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tseries)
library(forecast)
library(vars)
library(GGally)
#library(tseries)
#library(forecast)
```


```{r}
rm(list=ls())
getwd()
setwd("/Users/annevalder/Desktop/UNI /WU WIEN/SoSe_21/MA/R-CODE")
```
# Load all data in (GDP, CCI, Goolge), extension unemployment (statAT), inflation(statAT), uncertainty index (paper Brown et al)

#GDP
```{r}
t_0 <- as.Date("2006-01-01") #Setting start date for the series
t_1 <- as.Date("2021-07-01") #Setting end date for the series
t_2 <- as.Date("2021-12-31")
Date_q <- seq.Date(t_0,t_1,by="quarter")
Date_m <- seq.Date(t_0,t_2,by="month")

bigT <- length(Date_q)
M <- 6
Data   <- matrix(NA,bigT,M)
colnames(Data)  <- c("gdp", "cci", "google","gdp_ts", "cci_ts", "google_ts") 
Data <- as.data.frame(Data)
rownames(Data)  <- as.Date(Date_q)

# abgerufen final am 11.02.2022
#Source: https://data.oecd.org/gdp/quarterly-gdp.htm#indicator-chart
gdp <- read.csv("https://raw.githubusercontent.com/anneval/MA_data/main/RData/GDP_oecd.csv")[9:71,6:7]

colnames(gdp)[colnames(gdp) %in% c("TIME", "Value")] <- c("QDate", "value")
gdp <- cbind(gdp,Date_q)
#ts:
gdp_ts <- ts(gdp$value, start=c(2006,1), end = c(2021,3), frequency = 4)

Data[(Date_q%in%gdp$Date_q),1] <- gdp$value
Data$gdp_ts <- ts(Data$gdp, start=c(2006,1), end = c(2021,3), frequency = 4)
```
#CCI
```{r}
# abgerufen final am 11.02.2022
#Source: https://ec.europa.eu/info/business-economy-euro/indicators-statistics/economic-databases/business-and-consumer-surveys/download-business-and-consumer-survey-data/time-series_en#consumers

cci <- read.csv2("https://raw.githubusercontent.com/anneval/MA_data/main/RData/CCI.csv")[253:444,c(1,297)]

colnames(cci)[colnames(cci) %in% c("X", "CONS.AT.TOT.COF.BS.M")] <- c("time", "value")
cci <- cbind(cci,Date_m)
cci <- arrange(cci, Date_m)
cci$Date_q <- as.yearqtr(cci$Date_m) 
cci$Date_q <- as.Date(cci$Date_q)

#cci$Date_q  <- as.character(cci$Date_m)
#cci$Date_q[CCI_mntly$Date_q == "2021 Q4"] <- "2021 Q3"

cci_qtrly <- cci %>% group_by(Date_q) %>%
  summarise_all(mean)

cci_qtrly_63 <- cci_qtrly[1:63,]
Data[(Date_q%in%cci_qtrly_63$Date_q),2] <- (cci_qtrly_63$value)
Data$cci_ts <- ts(Data$cci, start=c(2006,1), end = c(2021,3), frequency = 4)
```

# Google
```{r}
# abgerufen: daily update
Google_AT_daily <- read.csv("https://raw.githubusercontent.com/anneval/MA_data/main/raw/at/trendecon_sa.csv")

# transform to quarterly and then do ts:
Google_AT_daily$time <- as.Date(Google_AT_daily$time)
Google_AT_daily <- arrange(Google_AT_daily, time)
Google_AT_daily$Date_q <- as.yearqtr(Google_AT_daily$time)
Google_AT_daily$Date_q <- as.Date(Google_AT_daily$Date_q)
#Google_AT_daily$QDate  <- as.character(Google_AT_daily$QDate)
#Google_AT_daily$QDate[Google_AT_daily$QDate == "2022 Q1"] <- "2021 Q3"  
#Google_AT_daily$QDate[Google_AT_daily$QDate == "2021 Q4"] <- "2021 Q3"  
Google_AT_qtrly <- Google_AT_daily %>% group_by(Date_q) %>%
  summarise_all(mean)

#rownames(Google_AT_qtrly)  <- as.Date(Date_q)
Google_AT_qtrly_63 <- Google_AT_qtrly[1:63,]
Data[(Date_q%in%Google_AT_qtrly_63$Date_q),3] <- (Google_AT_qtrly_63$value)
Data$google_ts <- ts(Data$google, start=c(2006,1), end = c(2021,3), frequency = 4)
```


#ADF tests: 
```{r}
adf.test(Data$gdp) # stationary
adf.test(Data$cci) # stationary
adf.test(Data$google) # not stat # test agao  after remove inlation 
# not stationary since I included inflation... 

adf.test(diff(Data$gdp))
adf.test(diff(Data$cci))
adf.test(diff(Data$google))
```
kss tests and phillip peron tests:
```{r}
kpss.test(Data$gdp) # stationary
kpss.test(Data$cci) # stationary
kpss.test(Data$google) # stationary

pp.test(Data$gdp) # stationary
pp.test(Data$cci) # not stationary
pp.test(Data$google) # stationary
```
# ACF and PACF plots: Reasoning stationarity
```{r}
par(mfrow = c(2,2))
acf(Data$gdp)
acf(Data$cci)
acf(Data$google)

par(mfrow = c(2,2))
pacf(Data$gdp)
pacf(Data$cci)
pacf(Data$google)
```
# Correlations
```{r}
ggpairs(Data[,1:3])
```

#### Models - simple first later expanding? (and CV, robustness)

```{r}
Data_train <- Data[1:44,] # 70%
Data_test <- Data[45:63,] #20%
```

# Models Heik:

AR(1)
Model 1: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \epsilon_t  \qquad  t =1,\dotsc,T$

```{r}
mod1 <- lm(gdp ~ lag(gdp), data = Data_train)
pred_mod1 <- predict(mod1, Data_test)
summary(mod1)
accuracy(mod1)
#forecast_mod1 <- predict(mod1, n.ahead = 3)
#plot(forecast_mod1)
```
Model 2:  $GDP_t = \beta_0 +\beta_1 CCI_{t} +\epsilon_t \qquad  t =1,\dotsc,T$
```{r}
mod2 <- lm(gdp ~ cci, data = Data_train)
pred_mod2 <- predict(mod2, Data_test)
summary(mod2)
accuracy(mod2)
```
Model 3:  $GDP_t = \beta_0 +\beta_1 GT_{t,i} +\epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$

```{r}
mod3<- lm(gdp ~ google, data = Data_train)
pred_mod3 <- predict(mod3, Data_test)
summary(mod3)
accuracy(mod3)
```
Model 4: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 CCI_{t} + \epsilon_t \qquad  t =1,\dotsc,T$

```{r}
mod4<- lm(gdp ~ lag(gdp) + cci, data = Data_train)
pred_mod4 <- predict(mod4, Data_test)
summary(mod4)
accuracy(mod4)
```
Model 5: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 GT_{t,i} + \epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$

```{r}
mod5<- lm(gdp ~ lag(gdp) + google, data = Data_train)
pred_mod5 <- predict(mod5, Data_test)
summary(mod5)
accuracy(mod5)
```

Model 6: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 CCI_{t} + \beta_3 GT_{t,i} + \epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$
```{r}
mod6<- lm(gdp ~ lag(gdp) + cci + google, data = Data_train)
pred_mod5 <- predict(mod6, Data_test)
summary(mod6)
accuracy(mod6)
```

#VAR model mit Varselect all 3 variables
```{r}
Yselect <- VARselect(Data_train[,1:3])               # computes information criteria
  lag <- Yselect$selection[1]  # AiC
  
VAR_est <- VAR(y = Data_train[,1:3], p = lag)
#summary(VAR_est$varresult$gdp)$adj.r.squared
#summary(VAR_est$varresult$cci)$adj.r.squared
#summary(VAR_est$varresult$google)$adj.r.squared
forecasts <- predict(VAR_est,Data_test[,1:3])
forecasts
plot(forecasts)

```
# Residual analysis
```{r}
sum2 <- data.frame(
  VAR_est$varresult$gdp$residuals,
  VAR_est$varresult$cci$residuals,
  VAR_est$varresult$google$residuals)

adf.test( VAR_est$varresult$gdp$residuals)
adf.test(VAR_est$varresult$cci$residuals)
adf.test(VAR_est$varresult$google$residuals)
```

# Normality test
```{r}
Normtest <- normality.test(VAR_est) # H0: normality
print(Normtest)
plot(Normtest)

```
# serial correlation check
```{r}
ser.test <- serial.test(var.UR) # H0: no serial correlation
ser.test
```

#VAR model mit Varselect all 3 variables
```{r}
Yselect <- VARselect(Data_train[,1:2])               # computes information criteria
  lag <- Yselect$selection[1]  
  
VAR_est <- VAR(y = Data_train[,1:3], p = lag)
#summary(VAR_est$varresult$gdp)$adj.r.squared
#summary(VAR_est$varresult$cci)$adj.r.squared
#summary(VAR_est$varresult$google)$adj.r.squared
forecasts <- predict(VAR_est,Data_test[,1:3])
accuracy(forecasts)

plot(forecasts)

```


# AR & VAR model Heik again mit selction criteria (AIC, autoarima, varselect)
```{r}
# AR(1) model 
fit <- auto.arima(Data_train[,"gdp"], trace = TRUE,  ic = c("aic"))

#pred <- predict(fit,Data_test) ?? 
plot(forecast(fit,h=3))
accuracy(fit)
```

#Extensions
```{r}

```


# Expanding window estimation instead of fixed test and train data set 

```{r}
expanding_window_OLS <- function(data, dep_var, start = 13){ # change to 12 
  expanding_OLS <- list() # empty vector
  predicted_OLS <- list()
  resid_OLS <- list()
  error_OLS <- c() 
  i <- 0
  for(t in start:nrow(data)){
    i <- i+1
    expanding_OLS[[i]] <- lm(formula = formula_ols_all_rev, data = data[1:(t-1),])
    resid_OLS[[i]] <- resid(expanding_OLS[[i]])
    predicted_OLS[i] <- predict(expanding_OLS, newdata = data[t,])
    error_OLS[i] <- as.numeric(predicted_OLS[[i]]- data[t,3]) # anpassen je nach dem welchs
    Summary_OLS <- list(expanding_OLS,predicted_OLS, resid_OLS,error_OLS)
  }
  return(Summary_OLS)
}
```



# transform to ts all,  aggregate data (new gdP!)
# do tests for stationarity
# create one data matrix for Var later
# write down all models  I want todo (Heik models, 4 lags quarterly data, and tests for lag length with arima and VAR models)

# do simple split OOS forecast and expanding OOS forecast, extension LLOCV
# adjust expanding window functions (for Var, arima)

# Robustness different aggregation 
