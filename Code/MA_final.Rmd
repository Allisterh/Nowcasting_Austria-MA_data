---
title: "MA_final"
author: "Anne Valder"
date: "1/20/2022"
output: html_document
---

TS indo doc quelle: https://ethz.ch/content/dam/ethz/special-interest/math/statistics/sfs/Education/Advanced%20Studies%20in%20Applied%20Statistics/course-material-1719/Zeitreihen/ATSA_Script.pdf

Take CCI in quarterly and do adf dann! 
Best results of log abs GDP!

DO: 

use ACF for stationarity argument, and AR(1) selection, pattern suggets AR provess from ACF and then use PACF to determine lag length "Alternating positive and negative, decaying to zero	Autoregressive model. Use the partial autocorrelation plot to help identify the order."
ACF: 
Exponential, decaying to zero	Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model.

One or more spikes, rest are essentially zero (or close to zero)	Moving average model, order identified by where plot becomes zero.

forecasting ... andere observationen 
 
write/ do models with gdp gr data?arguement ACF plot und andern beiden tests!
plot models all?
tests on model on all kinds of stuff

number of lags AR etc??? AR(4)!!!!! determine mit varselect, und autoarima

für AR models look at ACF and PACF


change data aggregation so that I have one, two data points more for forecast
do all as seperate TS? dann vllt problem mit Function und Loop??

Select lag level each individual for model?

plot model predicitions... 

nur relevant für y= y+x
not the y = x models!!! 


"The choice of the model order p then relies on the analysis of the sample PACF."Following the paradigm of parameter parsimony, we would first try the simplest model that seems plausible. This means choosing the smallest p at which we suspect a cut-off, i.e. the smallest after which none, or only few and weakly significant partial autocorrelations follow 
ar.ols(
fit.aic <- ar.burg(log(lynx))
fit.aic <- ar.burg(log(lynx))

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# schauen welche ich wirklich brauche!
#rm(list=ls())
library(readr)
library(readxl)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(Metrics)
library(gtrendsR)
library(tsbox)
library(zoo)
library(tseries)
library(tempdisagg)
library(x12)
library(trendecon)
library(pls)
library(prophet)
library(glmnet)
library(reshape2) 
library(midasr)
library(lubridate)  
library(caret)
library(forecast)
```

# Read in data (GDP & ConsumerConfidence & GT fertiger Indicator TrendEcon)
## GDP, Source: https://data.oecd.org/gdp/quarterly-gdp.htm#indicator-chart
#### GET DATA FROM STATISTIC AUSTRIA!!! 

```{r}
getwd()
setwd("/Users/annevalder/Desktop/UNI /WU WIEN/SoSe_21/MA/data/Rcode")

#### GDP ####
GDP_qtrly <- GDP_oecd[9:71,6:7] # unnötige columns weg und nur AT daten!
colnames(GDP_qtrly)[colnames(GDP_qtrly) %in% c("TIME", "Value")] <- c("QDate", "Value")
#### Check data for stationarity and difference
GDP_GR_diff <- diff(GDP_qtrly$Value) 

adf.test(GDP_qtrly$Value) # not stationary growth rates!!! , H0 not stat, cannot reject that is not stationary,null hypothesis (not stat) can not be rejected - 

#we cannot reject that data is not stationary, hence series is non-stat.

 
adf.test(GDP_GR_diff) # differnce of groeth rates is stationary! 
### change to TS objrects! 
GDP_ts = ts(GDP_qtrly$Value,start=c(2006), end = c(2022), frequency = 4)
pp.test(GDP_qtrly$Value) # statioanry H0 not stat
#get 0.01 so we can reject H0 that the series is non-stationary, thus is series is stationary


#opposite of ADF: Null Hypothesis: The process is trend stationary.
kpss.test(GDP_qtrly$Value) # statioanry, h0 stationarity, cannot reject the H0 cannot reject that is stationary, so is stationary 
library(urca)
ur.kpss(GDP_qtrly$Value)
ndiffs(GDP_qtrly$Value)
#Case 3: KPSS indicates stationarity and ADF indicates non-stationarity - The series is trend stationary. Trend needs to be removed to make series strict stationary. The detrended series is checked for stationarity.
diffTS <- diff(GDP_ts)

ts.plot(diffTS)
ts.plot(GDP_ts)

acf(GDP_qtrly$Value) #can also refer stationarity fro this!
acf(GDP_AT$logValue)
pacf(GDP_AT$logValue)
pacf(GDP_qtrly$Value) # suggests AR(1) model!!

fit <- auto.arima(GDP_qtrly$Value, trace = TRUE,  ic = c("aic"))  ## sugessts a MA(1) model !!
tsdisplay(resid(fit))
# As we can observe, the time series of residuals is not White Noise, since there are several ACF and PACF coefficients that exceed the confidence bands. Hence, the AIC-selected model clearly underfits these data. If an AR(10) is used in place of the ARMA(2,3), the residuals feature the desired White Noise property. As a conjecture, we would reject the ARMA(2,3) here despite its better AIC value and note that blindly trusting in automatic model selection procedures may well lead to models that fit poorly.The next step is to perform residual analysis – if the model is appropriate, they must look like White Noise.

#With a stationary ARMA( p, q) , we would here focus more on the long-term aspects of the series, i.e. the climatic changes that happen over decades or even centuries. If the data are differenced, we consider the changes in growth from year to year. This puts the focus on the bio-chemical aspect, while climate change is ruled out. Not surprisingly, the autocorrelation among the differenced data is strongly negative at lag 1. This means that a big positive change in growth is more likely to be followed by a negative change in growth and vice versa. Hence this model focuses more on the recovery of the tree after strong resp. weak growth in one year versus the next. Hence it would not primarily be the climate which is modelled, but more the bio-chemical processes within the tree. Thus, it is also a matter of the applied research question which of the two models is more suited

#Often one wishes to express the forecasting error for a time series model for understanding the magnitude of the deviations that we need to expect. Moreover, correctly implemented, forecasting errors can also serve for model choice. For truly evaluating the performance of a model, it is important to study the out-of-sample performance. This means that as above, the last part of the data need to be withheld from the fitting process. These values can then be forecasted and compared against the observed ones. Please note that we cannot rely on the insample resp. training error for such considerations. A good or even perfect training data fit can be achieved by overparametrizing a model, but this does not imply good forecasting performance, yet in fact overfitting is usually detrimental to the out-of-sample results.
# We also notice that the convergence of the forecast towards the global mean is much slower here than for the Beaver data. This is due to a much stronger signal-to-noise ration in the logged lynx data. However, for longer forecasting horizon k (resp. with increasing h in the R function), the predicted values would also converge to the global mean.

#In order to determine between an AR and a MA series an auto-correlation function (ACF) plot can be used. This is a plot of total correlation between different lag functions. Also the partial autocorrelation function (PACF) can be used alongside this. The PACF is the correlation between two lags irrespective of other lags in the series. 
#Each bar represents the correlation between the original series and its kth lag. The first bar is always equal to one as this is simply measuring the variable correlated with itself. The blue line apparent in any plot represents statistically significant values other than 0, meaning any bars underneath this threshold are not statistically significant. The method to distinguish between AR and MA processes comes from observing which graph falls below this line first. If the PACF graph becomes insignificant before the ACF plot then the series is mostly an AR process. For instance, in the top two images the PACF becomes insignificant after the second lag, meaning it is an AR(2) process. On the other hand in the bottom images the ACF plot becomes insignificant after the 2nd bar, meaning it is mostly a MA(2) process.
ts.plot(GDP_ts_AT)
ts.plot(GDP_ts)

plot(GDP_AT$Value)
plot(decompose(GDP_ts))

lag.plot(GDP_qtrly$Value,do.lines=FALSE, pch=20)
lag1 <-lag(GDP_qtrly$Value)
           
cor.test(GDP_qtrly$Value,lag1)
tsdisplay(GDP_ts, points=FALSE)


abs(polyroot(c(0.4115,-0.2963))) # numbers are coefficients

fit <- arima(diff(attbond), order=c(0,0,1))
#plot(resid(fit))
#qqnorm(resid(fit)); qqline(resid(fit))
#acf(resid(fit)); pacf(resid(fit)

# get all errors.. 
round(accuracy(forecast(modelols_Google, h=14), btest),3)
```

#### GDP from Statistik Austria in MIL EUR

```{r}
GDP_AT <- read_xlsx("~/Desktop/Uni/WU WIEN/SoSe_21/MA/data/GDP_STATAT.xlsx",sheet = "Data Sheet 0")[50:112,2:3]

colnames(GDP_AT)[colnames(GDP_AT) %in% c("...2", "...3")] <- c("QDate", "Value")
GDP_AT$Value <- as.numeric(GDP_AT$Value)
adf.test(GDP_AT$Value) # statioanryy at 5% level 
kpss.test(GDP_AT$Value) # can rejrect h0 so stationary... 
pp.test(GDP_AT$Value)
# take log, of course same stat resutls
GDP_AT$logValue <- log(GDP_AT$Value)
adf.test(GDP_AT$logValue)

kpss.test(GDP_AT$Value, null = "Trend") # is trend stationary
kpss.test(GDP_AT$logValue, null = "Trend") # is trend stationary

## calculate growth rate manually with log diff!! 
GDP_AT_diff <- diff(GDP_AT$logValue, difference=1)
adf.test(GDP_AT_diff) ## not stationary!!!! SAME AS ABOVE 
kpss.test(GDP_AT_diff) # stationary 
pp.test(GDP_AT_diff) # stationary 
acf(GDP_AT_diff)
GDP_AT_diff_2 <- diff(GDP_AT$logValue, difference=2) # model the “change in the changes” of the original dat

GDP_ts_AT <- ts(GDP_AT$Value,start=c(2006), end = c(2021), frequency = 4)

adf.test(GDP_AT_diff_2) # stationary after differencing twice! 

### Calculate growth rate not over approximation and log!! 
GDP_AT$GR <- ((GDP_AT$Value-lag(GDP_AT$Value))/lag(GDP_AT$Value))*100
adf.test(GDP_AT$GR[2:63]) # same as with log diff! not stationary
GDP_AT_GR_diff<- diff(GDP_AT$GR)

GDP_AT_GR_diff <- as.data.frame(GDP_AT_GR_diff)
GDP_AT_GR_diff <- GDP_AT_GR_diff[2:62,]
adf.test(GDP_AT_GR_diff) # stationary again!!! 

GDP_AT$GR_qtrly <-GDP_qtrly$Value

GDP_AT$GR_diff <- as.integer(GDP_AT$GR-GDP_AT$GR_qtrly,4)

plot(GDP_AT_diff)
plot(GDP_AT_diff_2)

acf(GDP_AT$Value, lag.max = 30)
pacf(GDP_AT$Value)
acf(GDP_qtrly$Value)
acf(GDP_AT_GR_diff)

acf(CCI_mntly$Value)
acf(CCI_qtrly$Value)
pacf(CCI_qtrly$Value)

#The above ACF is “decaying”, or decreasing, very slowly, and remains well above the significance range (dotted blue lines). This is indicative of a non-stationary series.

acf(CCI_diff) # use CCI diff !!! 

acf(Google_AT_qtrly$Value)


# types of stationarity
#https://www.statisticshowto.com/stationarity/

```
### Consumer Confidence data ###
##Source: https://ec.europa.eu/info/business-economy-euro/indicators-statistics/economic-databases/business-and-consumer-surveys/download-business-and-consumer-survey-data/time-series_en#consumers

```{r}
CCI_mntly<- read_xlsx("/Users/annevalder/Desktop/Uni/WU WIEN/SoSe_21/MA/data/CCI.xlsx",sheet = "CONSUMER MONTHLY")[253:444,c(1,297)] # 297 ist Spalte KK - total index!

CCI_mntly <- CCI_mntly %>% data.frame() 
colnames(CCI_mntly)[colnames(CCI_mntly) %in% c("...1", "CONS.AT.TOT.COF.BS.M")] <- c("Date", "Value")
CCI_mntly$Value <- as.numeric(CCI_mntly$Value)
#CCI_mntly_ts <- ts(CCI_mntly$Value)
adf.test(CCI_mntly$Value) # not stationary!
CCI_diff <- diff(CCI_mntly$Value)
adf.test(CCI_diff) ## differneced then stationary!!!! 
```

### Create quarterly aggregate of CCI data 
```{r}
#### Convert Monthly data into quarterly data! 
##1) Temporal Aggregation!
CCI_mntly <- arrange(CCI_mntly, Date)
CCI_mntly$QDate <- as.yearqtr(CCI_mntly$Date)
CCI_mntly$QDate <- as.character(CCI_mntly$QDate)
CCI_mntly$QDate[CCI_mntly$QDate == "2021 Q4"] <- "2021 Q3"
#CI_mntly$QDate <- as.yearqtr(CCI_mntly$QDate)
CCI_qtrly <- CCI_mntly %>% group_by(QDate) %>%
  summarise_all(mean)
adf.test(CCI_qtrly$Value) # 0.01

acf(CCI_qtrly$Value)

##2) systematic sampling.
CCI_qtrly_M <- CCI_mntly %>% group_by(QDate) %>%
  filter(Date==max(Date)) %>% 
  ungroup 
adf.test(CCI_qtrly_M$Value) # 0.02

#### Quarterly of CCI differenced data 
# DO AS ROBUSTNESS # 62 observations...
```

### Create quarterly aggregate of CCI diff data
```{r}
##1) Temporal Aggregation!
CCI_diff <- as.data.frame(diff(CCI_mntly$Value))
CCI_diff <- cbind.data.frame(CCI_mntly$Date[2:192],CCI_diff)
colnames(CCI_diff)[colnames(CCI_diff) %in% c("CCI_mntly$Date[2:192]", "CCI_diff")] <- c("Date", "Value")

CCI_diff <- arrange(CCI_diff, Date)
CCI_diff$QDate <- as.yearqtr(CCI_diff$Date)
CCI_diff$QDate <- as.character(CCI_diff$QDate)
CCI_diff$QDate[CCI_diff$QDate == "2021 Q4"] <- "2021 Q3"
#CI_mntly$QDate <- as.yearqtr(CCI_mntly$QDate)
CCI_diff_qtrly <- CCI_diff %>% group_by(QDate) %>%
  summarise_all(mean)

adf.test(CCI_diff_qtrly$`diff(CCI_mntly$Value)`) # 0.01
acf(CCI_diff_qtrly$`diff(CCI_mntly$Value)`)
```

## Google Data 
```{r}
# change to source data from my github! 
Google_AT_daily <- read.csv("https://raw.githubusercontent.com/trendecon/data/master/data/at/trendecon_sa.csv")
Google_AT_daily <-  data.frame(Google_AT_daily) 
colnames(Google_AT_daily)[colnames(Google_AT_daily) %in% c("time", "value")] <- c("Date", "Value")

adf.test(Google_AT_daily$Value) ## stationary 0.01
acf(Google_AT_daily$Value)

difftest <- diff(Google_AT_daily$Value)
acf(difftest)
adf.test(difftest)
kpss.test(Google_AT_daily$Value)
kpss.test(difftest)
```

### Create quarterly aggregate of Google data 
```{r}
Google_AT_daily$Date <- as.Date(Google_AT_daily$Date)
Google_AT_daily <- arrange(Google_AT_daily, Date)
Google_AT_daily$QDate <- as.yearqtr(Google_AT_daily$Date)
Google_AT_daily$QDate  <- as.character(Google_AT_daily$QDate)

Google_AT_daily$QDate[Google_AT_daily$QDate == "2022 Q1"] <- "2021 Q3"  
Google_AT_daily$QDate[Google_AT_daily$QDate == "2021 Q4"] <- "2021 Q3"  
Google_AT_daily$QDate <- as.yearqtr(Google_AT_daily$QDate)
Google_AT_qtrly <- Google_AT_daily %>% group_by(QDate) %>%
  summarise_all(mean)

adf.test(Google_AT_qtrly$Value) # stationary at 5%
 ### 2) Select the last day of the artificial Quarter!
#systematic sampling
acf(Google_AT_qtrly$Value) ## lag(4)

acf(Google_AT_daily$Value) # stat 

Google_AT_qtrly_M <- Google_AT_daily %>% group_by(QDate) %>%
  filter(Date==max(Date)) %>% 
  ungroup 
adf.test(Google_AT_qtrly_M$Value) # stationary at 0.01
acf(Google_AT_qtrly_M$Value) # stat



```
### Create one data frame 1
```{r}
M <- 8
t_0 <- as.Date("2006-01-01") #Setting start date for our series
t_0_2 <- as.Date("2009-01-01") 
t_1 <- as.Date("2021-09-30") #Setting end date for our series
Date <- seq.Date(t_0,t_1,by="quarter") # set for quarterly data 
Date2 <- seq.Date(t_0_2,t_1,by="quarter")
bigT <- length(Date)
  
Data   <- matrix(NA,bigT,M)    # rows are length of t, columns are numbers of variables from FRED 
colnames(Data)  <- c("QDate","GDP","GDPGR", "CCI_TA","CCI_diff", "CCI_SyS","Google_TA","Google_SyS") 

Data <- as.data.frame(Data)
rownames(Data)  <- as.character(Date)
Data$QDate <- as.yearqtr(CCI_qtrly$Date)
```

### Create one data frame 2
```{r}
GDP_AT <- cbind(GDP_AT,Date)
GDP_qtrly <- cbind(GDP_qtrly,Date)
CCI_qtrly$Date <- Date
CCI_diff_qtrly$Date <- Date
CCI_qtrly_M$Date <- Date
Google_AT_qtrly$Date <- Date
Google_AT_qtrly_M$Date <- Date

Data[(Date%in%GDP_AT$Date),2] <- (GDP_AT$Value)
Data[(Date%in%GDP_qtrly$Date),3] <- (GDP_qtrly$Value)
Data[(Date%in%CCI_qtrly$Date),4] <- (CCI_qtrly$Value) 
Data[(Date%in%CCI_qtrly$Date),5] <- (CCI_diff_qtrly$`diff(CCI_mntly$Value)`) 
Data[(Date%in%CCI_qtrly_M$Date),6] <- (CCI_qtrly_M$Value)
Data[(Date%in%Google_AT_qtrly$Date),7] <- (Google_AT_qtrly$Value)
Data[(Date%in%Google_AT_qtrly_M$Date),8] <- (Google_AT_qtrly_M$Value)
###### test for stat here and differnece then?
#Data$GDP_lag <- lag(Data$GDP)
Data$GDP_lag <- lag(GDP_AT$logValue)  ###### 62 observationen...m 
Data$GDP_GR_lag <- lag(Data$GDPGR)
```

##### Descriptives

```{r}
adf.test(Data$GDP)
adf.test(diff(Data$GDP))
kpss.test(Data$GDP)
kpss.test(diff(Data$GDP))
acf(Data$GDP)
pacf(Data$GDP)
acf(diff(Data$GDP))
pacf(diff(Data$GDP))

adf.test(Data$CCI_TA)
kpss.test(Data$CCI_TA)
acf(Data$CCI_TA)
pacf(Data$CCI_TA)

adf.test(Data$CCI_SyS)
kpss.test(Data$CCI_SyS)
acf(Data$CCI_SyS)
pacf(Data$CCI_SyS)

adf.test(Data$Google_TA)
kpss.test(Data$Google_TA)
acf(Data$Google_TA)
pacf(Data$Google_TA)

adf.test(Data$Google_SyS)
kpss.test(Data$Google_SyS)
acf(Data$Google_SyS)
pacf(Data$Google_SyS)


require(fUnitRoots)
adfTest(Data$GDP, type = "nc")
adfTest(GDP_qtrly$Value, type = "c")
adfTest(GDP_qtrly$Value, type = "ct")
adfTest(y, lags = 1, type = c("nc", "c", "ct"))

pp.test(Data$GDP)


library(forecast)
auto.arima(GDP_qtrly$Value, trace = TRUE)  # gets 280 als AIC....
auto.arima(Data$CCI_TA, trace = TRUE) 
auto.arima(Data$GDP, xreg = x ,trace = TRUE) 


ar1 <- arima(GDP_ts, order = c(1,0,0)) # get 260 als AIC
ar2 <- arima(GDP_ts, order = c(0,0,0))
ar3 <- arima(GDP_ts, order = c(0,0,1)) #g gets alsi here bezzer resluts
ar4 <- arima(GDP_ts, order = c(4,0,0))
ar5 <- arima(GDP_ts, order = c(5,0,0))
ar6 <- arima(GDP_ts, order = c(6,0,0))
ar7 <- arima(GDP_ts, order = c(7,0,0))
ar8 <- arima(Data$GDPGR, order = c(8,0,0))
ar9 <- arima(Data$GDPGR, order = c(10,0,0))

MA <- arima(GDP_ts, order = c(0,0,1))

AIC <- AIC(ar1, ar2, ar3, ar4)
BIC <- BIC(ar1, ar2, ar3, ar4)
#### VAR selction !!!! 

x <- Data[,c(3,4,7)] # GDP; CCI; and GOOGLe 
x <- as.matrix(x)
library(vars)

Yselect <- VARselect(x)               # computes information criteria
  lag <- Yselect$selection[2]  

lag

VAR_est <- VAR(y = x, p = lag)
summary(VAR_est$varresult$GDPGR)$adj.r.squared
summary(VAR_est$varresult$CCI_TA)$adj.r.squared
summary(VAR_est$varresult$Google_TA)$adj.r.squared
forecasts <- predict(VAR_est)
forecasts
plot(forecasts)
 ```
### Graphical representation of data 

# Google
```
 
```{r}
Google_AT_daily <- Google_AT_daily[,c(1,2)]
tsbox::ts_plot(Google_AT_daily, title = "Austria")
```
see other FILE !

##### Decision on model VAR and VECM and how many lags - macro slides! 

```{r}
library(GGally)
ggpairs(Data[,c(2,3,4,5,7)])
```

## Models:
test just basic OLS no CV
```{r}
model_ols1 # wesetnlich bessere Ergebnisse so!!! 

library(dynlm)
model_ols1 <- dynlm(GDP ~ GDP_lag, data = Data)
summary(model_ols1)

 model_ols2 <- dynlm(GDPGR ~ GDP_GR_lag, data = Data) #same as 4
summary(model_ols2)

model_ols3 <- lm(GDP_GR_diff ~ lag(GDP_GR_diff)) #same as 5
summary(model_ols3)

model_ols4 <- lm(GDP_AT_diff ~ lag(GDP_AT_diff)) #same as 2
summary(model_ols4)

model_ols5 <- lm(GDP_AT_diff_2 ~ lag(GDP_AT_diff_2)) #same as 3
summary(model_ols5)

sqrt(mean(model1$residuals^2))
test <- predict(model1$fitted.values)
plot(test)
rmse(Data$GDP, model1$fitted.values)

library(forecast)
fc_goog <- snaive(GDP_ts, 10)
fc_goog
autoplot(fc_goog)
checkresiduals(fc_goog)
```

CV mit one test train set - like this: "does not account for possible changes in the structure of the data over the time because you have only one estimation of the model". Therefore expanding window 

```{r}
Data_train <- Data[1:44,] # 70%
Data_test <- Data[45:63,] #20%

Data_train <- Data[1:13,] # 70%
Data_test <- Data[14,] #20%

model1_CV <- lm(GDP ~ GDP_lag, data = Data_train)
summary(model1_CV)
pred_CV <- predict(model1_CV, Data_test)
```
The ACF plots the correlation coefficient against the lag, which is measured in terms of a number of periods or units. A lag corresponds to a certain point in time after which we observe the first value in the time series.To summarize, autocorrelation is the correlation between a time series (signal) and a delayed version of itself, while the ACF plots the correlation coefficient against the lag, and it’s a visual representation of autocorrelation.


Partial autocorrelation is a statistical measure that captures the correlation between two variables after controlling for the effects of other variables. For example, if we’re regressing a signal S at lag t (S_{t}) with the same signal at lags t-1, t-2 and t-3 (S_{t-1}, S_{t-2}, S_{t-3}), the partial correlation between S_{t} and S_{t-3} is the amount of correlation between S_{t} and S_{t-3} that isn’t explained by their mutual correlations with S_{t-1} and S_{t-2}.To summarize, a partial autocorrelation function captures a “direct” correlation between time series and a lagged version of itself.

Stat
S_{t} has a constant mean.
S_{t} has a constant standard deviation.
There is no seasonality in S_{t}. If S_{t} has a repeating pattern within a year, then it has seasonality.
Augmented Dickey-Fuller Test (ADF) with the null hypothesis that the signal is non-stationary.
Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS) with the null hypothesis that the signal is stationary.

The ARMA(p, q) model is a time series forecasting technique used in economics, statistics, and signal processing to characterize relationships between variables. This model can predict future values based on past values and has two parameters, p and q, which respectively define the order of the autoregressive part (AR) and moving average part (MA).But before that, we need to know that both AR and MA models require the stationarity of the signal. Usually, using non-stationary time series in regression models can lead to a high R-squared value and statistically significant regression coefficients. These results are very likely misleading or spurious.It’s because there is probably no real relationship between them and the only common thing is that they’re growing (declining) over time. We can test that by computing the correlation between two random walks defined by the formula

The autoregressive model is a statistical model that expresses the dependence of one variable on an earlier time period. It’s a model where signal S_{t} depends only on its own past values. For example, AR(3) is a model that depends on 3 of its past values and can be written as

(5)   \begin{align*} S_{t} =\beta_{0} + \beta_{1}S_{t-1} + \beta_{2}S_{t-2} + \beta_{3}S_{t-3} + \epsilon_{t}, \end{align*}

where \beta_{0}, \beta_{1}, \beta_{2}, \beta_{3} are coefficients and \epsilon_{t} is error. We can select the order p for AR(p) model based on significant spikes from the PACF plot. One more indication of the AR process is that the ACF plot decays more slowly.For instance, we can conclude from the example below that the PACF plot has significant spikes at lags 2 and 3 because of the significant PACF value. In contrast, for everything within the blue band, we don’t have evidence that it’s different from zero. Also, we could try for p other values of lag that are outside of the blue belt. To conclude, everything outside the blue boundary of the PACF plot tell us the order of the AR model:

The MA(q) model calculates its forecast value by taking a weighted average of past errors. It has the ability to capture trends and patterns in time series data. For example, MA(3) for a signal S_{t} can be formulated as

(6)   \begin{align*} S_{t} = \mu + \epsilon_{t} + \gamma_{1}\epsilon_{t-1} + \gamma_{2}\epsilon_{t-2} + \gamma_{3}\epsilon_{t-3} \end{align*}

where \mu is the mean of a series, \gamma_{1}, \gamma_{2}, \gamma_{3} are coefficients and \epsilon_{t}, \epsilon_{t-1}, \epsilon_{t-2}, \epsilon_{t-3} are errors that have a normal distribution with mean zero and standard deviation one (sometimes called white noise).

In contrast to the AR model, we can select the order q for model MA(q) from ACF if this plot has a sharp cut-off after lag q. One more indication of the MA process is that the PACF plot decays more slowly.

Similar to selecting p for the AR  model, in order to select the appropriate q order for the MA model, we need to analyze all spikes higher than the blue area. In that sense, from the image below, we can try using q=6 or q=3.

Also, in order to find the best combination of p and q, we need to have some objective function that will measure model performance on a validation set.  Usually, we can use AIC and BIC for that purpose. The lower the value of these criteria, the better the model is.

ETH: this behavior is typical: the PACF of any
MA(q) process shows an exponential decay, while the ACF has a cut-off. In this respect, MA(q) processes are in full contrast to the AR( p) ’s, i.e. the appearance of ACF and PACF is swapped.

For verifying the quality of the fit, a residual analysis is mandatory. The residuals of the MA(1) are estimates of the innovations Et . The model can be seen as adequate if the residuals reflect the main properties of the innovations. Namely, they should be stationary and free of any dependency, as well as approximately Gaussian. We can verify this by producing a time series plot of the residuals, along with their ACF and PACF, and a Normal QQ-Plot.
There are no autocorrelations or partial autocorrelations that exceed the confidence bounds, hence we can safely conjecture that the residuals are not correlated and hence, all the dependency signal has been captured by the MA(1) . When inspecting the time series of the residuals, it seems stationary. However, what catches the attention is the presence of three positive outliers and the fact that the residuals are generally long-tailed. We might try to overcome this problem by a log-transformation,


In R, this is conveniently possible by using function auto.arima() from library(forecast). However, handle this with care: the function will always identify a “best fitting” ARMA(p,q) model, but it is of course not guaranteed that it fits the data well. Moreover, usage of the function is somewhat tricky, as many arguments need being set.

$fit <- auto.arima(log(sunspotarea), max.p=10, max.q=10,
         stationary=TRUE, seasonal=FALSE, ic="aic",
         stepwise=FALSE)$


Do Grid search for p and q order 

```{r}

require(tseries)
require(forecast)
require(graphics)
auto.arima(Data$GDPGR, trace = TRUE)


msft_ar <- arima(Data$GDPGR , order = c(4, 0, 0))
summary(msft_ar)
msft_ar

ar4_AT <- ar((Data$GDP),p = 4)
summary(ar4_AT)
ar4_AT$asy.var.coef


ar4_GR <- ar((Data$GDPGR),p = 4)
summary(ar4_GR)
ar4_GR$asy.var.coef

#VAR(Data$GDP,p = 4)

library(car) # autocorrelation errors
durbinWatsonTest(model_ols1)

adf.test(model1$residuals) # stat...

library(qcc)
q <- cumsum(Data[,2:6])
summary(q)
plot(q)
```


```{r}
modelols_Google <- lm(GDP_qtrly$Value ~ lag(GDP_qtrly$Value))
summary(modelols_Google)

modelols_Google2 <- lm(GDPGR ~ Google_TA, data = Data)  # best... 
summary(modelols_Google2)

modelols_Google3 <- lm(GDP_GR_diff ~ Google_TA[2:63], data = Data)
summary(modelols_Google3)
```
CCI 
```{r}
modelols_CCI1 <- lm(GDP ~ CCI_TA, data = Data)   # better !!!!!!
summary(modelols_CCI1)

modelols_CCI1_2 <- lm(GDP ~ CCI_SyS, data = Data)   #  !!!!!!
summary(modelols_CCI1_2)

modelols_CCI1_3 <- lm(GDP ~ CCI_diff_qtrly$Value, data = Data)   #  not better !!!!!!
summary(modelols_CCI1_3)

modelols_CCI2 <- lm(GDPGR ~ CCI_TA, data = Data)
summary(modelols_CCI2)

modelols_CCI3 <- lm(GDP_GR_diff ~ CCI_TA[2:63], data = Data)
summary(modelols_CCI3)
```

# FUNCTIONS 
# Als robustness CV statt expanding window... 
```{r}
ctrl <- trainControl(method = "LOOCV")
model1_CV <- train(GDP ~ CCI_TA, data = Data, method = "lm", trControl = ctrl)
print(model1_CV)
```
########################################################
## OLS MIT EXPANDING WINDOW!!!

# mit predict und expanding window, and errors, do OLS da eh nur AR(1) or??? 

Models Rep Paper!
AR(1)
Model 1: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \epsilon_t  \qquad  t =1,\dotsc,T$
Estimation Strategy: 

OLS
Model 2:  $GDP_t = \beta_0 +\beta_1 CCI_{t} +\epsilon_t \qquad  t =1,\dotsc,T$
Estimation Strategy: 

OLS
Model 3:  $GDP_t = \beta_0 +\beta_1 GT_{t,i} +\epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$
Estimation Strategy: 

AR(1)+OLS, check varselect
Model 4: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 CCI_{t} + \epsilon_t \qquad  t =1,\dotsc,T$
Estimation Strategy:

AR(1)+OLS,check varselect
Model 5: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 GT_{t,i} + \epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$
Estimation Strategy: 

AR(1)+OLS,check varselect
Model 6: $GDP_t = \beta_0 +\beta_1 GDP_{t-1} + \beta_2 CCI_{t} + \beta_3 GT_{t,i} + \epsilon_t \qquad  t =1,\dotsc,T \qquad  n =1,\dotsc,N$
Estimation Strategy: 

+ Models I want to do, relying on statistical tests????! 
AR(4)??? PACF, var select (mult TS), Autoarima (4,0,4) sugeested by GDP 

```{r}
# VAR_EQ1 <- dynlm(GDPGrowth ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2), 

expanding_window_OLS <- function(data, dep_var, start = 13){ # change to 12 
  expanding_OLS <- list() # empty vector
  predicted_OLS <- list()
  resid_OLS <- list()
  error_OLS <- c() 
  i <- 0
  for(t in start:nrow(data)){
    i <- i+1
    expanding_OLS[[i]] <- lm(formula = formula_ols_all_rev, data = data[1:(t-1),])
    resid_OLS[[i]] <- resid(expanding_OLS[[i]])
    predicted_OLS[i] <- predict(expanding_OLS, newdata = data[t,])
    error_OLS[i] <- as.numeric(predicted_OLS[[i]]- data[t,3]) # anpassen je nach dem welchs
    Summary_OLS <- list(expanding_OLS,predicted_OLS, resid_OLS,error_OLS)
  }
  return(Summary_OLS)
}
```

#Model 1
```{r}
# test vgl ols hand written 4 lags mit arima hier 
auto.arima(Data$GDPGR)

AR_GDP4 <- arima(Data$GDPGR, order = c(4,0,4))
summary(AR_GDP4)
pred <- predict(AR_GDP4) # forecast 1 step ahead!!! 

pred
ARIMA_GDP4 <- arima(Data$GDPGR, order = c(1,1,1))
summary(AR_GDP4)


AR_GDP1 <- arima(Data$GDPGR, order = c(1,0,0))
summary(AR_GDP1)

indep_var <- c("GDP_lag")
formula_ols_all_rev = GDP ~ GDP_lag

expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13) # muss datenset cutten weg LAG! nicht 13 damit erstes saple 12 hat???

pred <- do.call(rbind.data.frame, expanding_OLS[[2]])
pred <- cbind(Date2,pred)

colnames(pred)[colnames(pred) %in% c("Date2", "c.11.3139532306837..11.3001541360649..11.293972121479..11.2974895805974..")] <- c("Date", "Value")


pred_ts <- as.ts(pred$Value)
ts_plot(pred_ts)
ts_plot(GDP_ts)
```

### Extract errors!
```{r}
# OOS error terms--------------------------------------------------------------
# OOS = If you are forecasting for an observation that was not part of the data sample - it is Out-Of-Sample forecast.
error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]

# for AIC ID only need one model! 
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2))  ### RMSE ????? sq()
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) ####???
MSE_OLS
# IS error terms----------------------------------------------------------------
# IS: If you are forecasting for an observation that was part of the data sample - it is in-sample forecast.
resid_OLS <- expanding_OLS[[3]]
final_resid_OLS <- resid_OLS[[length(resid_OLS)]]

MSE_OLS_IS    <- sqrt(mean(final_resid_OLS^2))
MSE_OLS_IS
```

#Model 2
```{r}
dep_var <- c("GDPGR")
indep_var <- c("GDP_GR_lag")
formula_ols_all_rev = GDPGR ~ GDP_GR_lag

expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13) # muss datenset cutten weg LAG! nicht 13 damit erstes saple 12 hat???

pred <- do.call(rbind.data.frame, expanding_OLS[[2]])
pred <- cbind(Date2,pred)

colnames(pred)[colnames(pred) %in% c("Date2", "c..3.26440824378507...2.63678639630861...1.45000961100761..0.0778559988812335..")] <- c("Date", "Value")

pred_ts <- as.ts(pred$Value,start=c(2006), end = c(2021), frequency = 4)
ts_plot(pred_ts)
##### Errors

error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2))  ### RMSE ????? sq()
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) ####???
MSE_OLS
resid_OLS <- expanding_OLS[[3]]
final_resid_OLS <- resid_OLS[[length(resid_OLS)]]

MSE_OLS_IS    <- sqrt(mean(final_resid_OLS^2))
MSE_OLS_IS
```
#Model 3

```{r}
# irgendwas falsch wegen error.to large
dep_var <- c("GDP")
indep_var <- c("Google_TA")
formula_ols_all_rev = GDP ~ Google_TA

expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13) # muss datenset cutten weg LAG! nicht 13 damit erstes saple 12 hat???

pred <- do.call(rbind.data.frame, expanding_OLS[[2]])
pred <- cbind(Date2,pred)

colnames(pred)[colnames(pred) %in% c("Date2", "c.80121.1458595531..80411.9302336694..80874.0945870095..80798.3360000626..")] <- c("Date", "Value")

pred_ts <- as.ts(pred$Value,start=c(2009), end = c(2021), frequency = 4)
ts_plot(pred_ts)
##### Errors

error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2))  ### RMSE ????? sq()
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) ####???
MSE_OLS
resid_OLS <- expanding_OLS[[3]]
final_resid_OLS <- resid_OLS[[length(resid_OLS)]]

MSE_OLS_IS    <- sqrt(mean(final_resid_OLS^2))
MSE_OLS_IS
```

#Model 4

```{r}
#!!!!!!!!!!!!!geht nicht weil diff nicht im datensatz,.. formel error 
dep_var <- c("GDP_GR_diff")
indep_var <- c("Google_TA")
formula_ols_all_rev = GDP_GR_diff ~ Google_TA[2:63]

expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13) # muss datenset cutten weg LAG! nicht 13 damit erstes saple 12 hat???

pred <- do.call(rbind.data.frame, expanding_OLS[[2]])
pred <- cbind(Date2,pred)

colnames(pred)[colnames(pred) %in% c("Date2", "")] <- c("Date", "Value")

pred_ts <- as.ts(pred$Value,start=c(2009), end = c(2021), frequency = 4)
ts_plot(pred_ts)
##### Errors

error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2))  ### RMSE ????? sq()
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) ####???
MSE_OLS
resid_OLS <- expanding_OLS[[3]]
final_resid_OLS <- resid_OLS[[length(resid_OLS)]]

MSE_OLS_IS    <- sqrt(mean(final_resid_OLS^2))
MSE_OLS_IS
```

#Model 5 CCI

```{r}
#!!!!!!!!!!!!!geht nicht weil diff nicht im datensatz,.. formel error 
dep_var <- c("GDPGR")
indep_var <- c("CCI_diff")
formula_ols_all_rev = GDPGR ~ CCI_diff

expanding_OLS <- expanding_window_OLS(Data, dep_var, start = 13) # muss datenset cutten weg LAG! nicht 13 damit erstes saple 12 hat???

pred <- do.call(rbind.data.frame, expanding_OLS[[2]])
pred <- cbind(Date2,pred)

colnames(pred)[colnames(pred) %in% c("Date2", "c.0.471794773636675..0.499796587843315..0.510777691453762..0.489913594593913..")] <- c("Date", "Value")

pred_ts <- as.ts(pred$Value,start=c(2009), end = c(2021), frequency = 4)
ts_plot(pred_ts)
##### Errors

error_OLS <- expanding_OLS[[4]]
error_OLS <-error_OLS[!is.na(error_OLS)]
MAE_OLS_OOS <- mean(abs(error_OLS))
MAE_OLS_OOS
RMSE_OLS_OOS    <- sqrt(mean(error_OLS^2))  ### RMSE ????? sq()
RMSE_OLS_OOS
MSE_OLS <- mean(error_OLS^2) ####???
MSE_OLS
resid_OLS <- expanding_OLS[[3]]
final_resid_OLS <- resid_OLS[[length(resid_OLS)]]

MSE_OLS_IS    <- sqrt(mean(final_resid_OLS^2))
MSE_OLS_IS
```

#TO DO 

# but forecast!!!
#to do get dates right! grpah assign direkt ts dates.. 
# hiniken paper 
# do in month.y...
# do daily incidcator with own github

### after models 
Homoskedasticity assumption
Zero-mean assumption:
No-autocorrelation assumption:
Normality assumption
structural shocks !!!

### Look at Correlation (done), linearity, normality, stationarity (half done, interpret), summaries

