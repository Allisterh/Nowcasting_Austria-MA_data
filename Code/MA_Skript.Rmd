---
title: "Daily Indicator"
author: "Anne Valder"
date: "2/9/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(trendecon)
library(tidyverse)
library(tsbox)

```


```{r}
#setwd("/Users/annevalder/Desktop/Uni/WU WIEN/SoSe_21/MA/GIT")
keywords <- c("Wirtschaftskrise", "Kurzarbeit", "arbeitslos") #Insolvenz
geo ="AT"

# only run once!
proc_keyword_init("Wirtschaftskrise","AT")
proc_keyword_init("Kurzarbeit","AT")#recruitment, mortgage
proc_keyword_init("arbeitslos","AT")
#DO TOMORROW!
proc_keyword_init("Insolvenz","AT")#bankruptcy
proc_keyword_init("Inflation","AT") #??? Preisanstieg, Energiepreise
#recession , ‘economic reforms’, and ‘debt stabilization’.

# also do as robustness check different words!

#Keywords: What makes Google Trends a powerful tool for economic predictions is its coverage of a large number of aspects of economic activity. Data about search behaviour can be informative about consumption (e.g. related to searches for “vehicles”, “households appliances”), labour markets (e.g.“recruitment”), housing (e.g. “real estate agency”, “mortgage”), business services (e.g.“venture capital”, “bankruptcy”), industrial activity (e.g. “maritime transport”, “agricultural equipment”) as well as economic sentiment (e.g. “recession”) and poverty (e.g. “food bank”). Signals about multiple facets of the economy can be aggregated to infer a timely picture of the macro economy.(OECD PAPER)
## ADD to Robustness:
#Reasons I stayed with keywords instead of topic/ category search - computational issues if consider Sampling problem porperly!!! OECD: the algorithm (a “neural network”) extracts relevant information from 250 Google Trends variables, that each aggregate information about searches by Google users for thousands a keywords
```


better directly as one!
```{r}
proc_index(keywords,"AT", "trendecon")
```

```{r}
#lapply(keywords, proc_keyword) # do stepwise? 
# Includes the followign functions: proc_keyword_latest,proc_combine_freq, proc_seas_adj
proc_keyword("arbeitslos","AT")
proc_keyword("Wirtschaftskrise","AT")
proc_keyword("Kurzarbeit","AT")
proc_keyword("Insolvenz","AT")
```

```{r}
# read in all data series from the last step (seas. adjust)
data <- read_keywords(keywords,geo = "AT", id = "seas_adj")
# Combine the adjusted key word series to one overall sentinment series using PCA!
  # make sure all keywords have the same span
data <- data %>%
    filter(time <= min(ts_summary(data)$end))

x_prcomp <- filter(ts_prcomp(data), id == "PC1") %>%
    select(-id) %>%
    ts_scale()

# determine PC sign based on average correlation with actual time series
values  <- mapply(getElement, split(data, data$keyword), "value")
corsign <- mean(cor(values, x_prcomp$value))

if(corsign < 0) {
    x_prcomp$value <- -x_prcomp$value
}

x_prcomp$value <- -x_prcomp$value

# Final Index
write_keyword(x_prcomp, geo, "sa")
ts.plot(x_prcomp$value)

```
# Daily updates anderes file

```{r}
#1-create a variable holding the set of keywords used for the series:
kw_econsent <- c(
  "Wirtschaftskrise",
  "Kurzarbeit",
  "arbeitslos"
)

proc_index(kw_econsent, "AT", "econsent")
indices_in_production <- c(
  "econsent"
)
```

